---
title: "Tidy Humanities Data"
author: "Matthew Lincoln"
date: "May 30, 2019"
output:
  beamer_presentation:
    df_print: kable
    slide_level: 2
    fig_width: 6
    fig_height: 3
    fig_caption: true
---

```{r opts, message=FALSE, warning=FALSE, echo=FALSE}
library(knitr)
library(kableExtra)
library(tidyverse)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(message=FALSE, warning=FALSE, echo=FALSE)
```

## Making tidy humanities data

```{r}
include_graphics("img/kondo.jpg")
```


## The problem

>- Text, network, and other quantitative analyses need data supplied in one neat table
>- All real-life historical data are more complicated than that.

## What we'll learn

- Structuring our sources as "tidy" data for future analysis
  - Handling dates
  - Categorization
  - Multiple values
  - Missing or uncertain data
- Example queries
- Practical exercise

## What is tidy data?

1. One variable per column
1. One observation per row
1. Consistent data types
1. If you can't do 1 and 2, that means you need an extra table

```{r raw_sales, include=FALSE}
messy_data <- tibble(
  acq_no = c("1999.32", "1908.54", "1955.32", "1955.33"),
  museum = c("Rijksmuseum", "Victoria & Albert", "British Museum", "Rijksmuseum"),
  artist = c("Studio of Rembrandt, Govaert Flinck", "Jan Vermeer", "Possibly Vermeer, Jan", "Hals, Frans"),
  date = c("after 1636", "c. 1650", "c. 1655", "16220"),
  medium = c("oil on canvas", "oil paint on panel", "oil on canvas", "oil on canvas, relined"),
  tags = c("religious, portrait", "domestic scene and women", "woman at window, portrait", "portraiture")
)

artworks <- tibble(
  acq_no = messy_data$acq_no,
  medium = rep("oil", 4),
  support = c("canvas", "panel", "canvas", "canvas"),
  cons_note = c(NA, NA, NA, "relined")
)
```

## Messy (but well-intentioned!) data

```{r}
messy_data %>% kable() %>% kable_styling(latex_options = "scale_down")
```

# Dates

---

There's no one true way to encode date information. It depends on your use-case.

1. "point" events? Or durations?
2. How precise are your sources?
3. How varied is that precision?

## Point vs. duration events

```{r}
tribble(
  ~date,
  "1660-03-01",
  "1661-05-20",
  "1661-12-05"
) %>% kable()
```


Point data specifies exactly one point in time; durations have a beginning and an end.

For point data, when in doubt, use `YYYY-MM-DD` format. It's an international standard, everything reads it.
---


```{r}
tribble(
  ~beginning, ~end,
  "1660-03-01", "1660-03-03",
  "1661-05-19", "1665-05-25",
  "1661-12-05", "1661-12-05"
) %>% kable()
```
For ranges, you'll want to have a start and end point specified using the same `YYYY-MM-DD` format.

---

```{r}
tribble(
  ~start_by, ~end_by, ~note,
  "1660-03-01", "1660-04-01", "Month precision",
  "1661-01-01", "1665-12-31", "Year precision",
  "1661-12-05", "1661-12-05", "Day precision",
  "1661-12-05", "1661-12-07", "Day ranges"
) %>% kable()
```

Depending on your context, it might be more accurate to think in terms of "no sooner than" and "no later than" rather than "beginning" and "end". This can be useful when you have varying precision of dates:

## But don't overdo it

```{r}
tribble(
  ~untidy, ~tidy_start, ~tidy_end,
  "19th century", "1800", "1899",
  "17th-18th c.", "1600", "1799",
  "1670s", "1670", "1679",
  "mid-1800s", "1830", "1870"
) %>% kable()
```
If your sources never have information down to the month or day, or you know that such precision isn't important, then just use a year or century marker. As long as you are _consistent_.


---
```{r}
tribble(
  ~untidy, ~tidy_century,
  "12th c.", 11,
  "10th century", 9,
  "200s", 2
) %>% kable()
```

The precision that's useful to you will be totally context-dependent. Don't give yourself more work than you need to.


## Applying to our original data

We need to make an executive decision about how we want to express "circa" or unbounded claims like "after".

```{r}
clean_data_dates <- tibble(
  acq_no = messy_data$acq_no,
  orig_date = messy_data$date,
  year_early = c(1636, 1645, 1650, 1622),
  year_late = c(1680, 1655, 1660, 1622)
)
clean_data_dates
```

Here, I've expanded "circa" to mean around 5 years before or after. "after 1636" could have a number of different meanings depending on the context - maybe we can limit it based on the last year of the studio's activity. 

## Gotchas

- If you're dealing with times, not just dates... then watch out for time zones. Python and R both have specialized libraries for these.
- When hand entering dates, make sure to validate the dates! You will inevitably enter `YYYY-02-31`, which doesn't exist.


# Categorization

## What concepts matter to you?

What bits of your free text fields could be systematized?

Figure out the _grammar_ of your data (how it fits together) and define a _vocabulary_ (what the individual pieces are)

**It's easy to combine things back together after collecting data; it's hard to split them out.**

---

## Example 1


```{r}
messy_data %>% select(acq_no, medium) %>% kable()
```
Here, `medium` contains info on:

>- painting medium (what it's painted _with_)
>- painting support (what it's painted _on_)
>- conservation techniques

## Example 1

```{r}
artworks %>% kable()
```
- separated different concepts into columns
- standardized vocabulary for each concept
  - keep the differences that are important, get rid of the ones that aren't

## Example 2

```{r}
messy_data %>% select(acq_no,artist) %>% kable()
```
>- artist column tells us more than just a name - it also has qualifiers.

## Example 2

One possibility:

```{r}
tribble(
  ~acq_no, ~artist_1_name, ~artist_1_qual, ~artist_2_name, ~artist_2_qual,
  "1999.32", "Rembrandt", "studio", "Govaert Flinck", NA,
  "1908.54", "Jan Vermeer", NA, NA, NA,
  "1955.32", "Jan Vermeer", "possibly", NA, NA,
  "1955.33", "Frans Hals", NA, NA, NA
) %>% kable() %>% kable_styling(latex_options = "scale_down")
```

- separate the qualifiers from the artist name
- standardize the names so the same person is spelled consistently

## Example 2

One possibility:

```{r}
tribble(
  ~acq_no, ~artist_1_name, ~artist_1_qual, ~artist_2_name, ~artist_2_qual,
  "1999.32", "Rembrandt", "studio", "Govaert Flinck", NA,
  "1908.54", "Jan Vermeer", NA, NA, NA,
  "1955.32", "Jan Vermeer", "possibly", NA, NA,
  "1955.33", "Frans Hals", NA, NA, NA
) %>% kable() %>% kable_styling(latex_options = "scale_down")
```

- Should we split out the first / last names too?
    - only if you need to for your research!
- Now we're dealing with complicated multiple values...
    
# Multiple Values

Spreadsheets look like they just hold one value per cell. Often our variables have a many-to-one or many-to-many relationship.

2 strategies for this:

>- use a delimiter (`;`, `,`, `|`) to put together quick small labels and tags into one cell together
>- for complicated info, you need a _related table_

## Example 1: delimiters

```{r}
messy_data %>% select(acq_no, tags) %>% kable()
```

These tags are self-contained (they don't have lots of related info - the term IS the data).

## Example 1: delimiters

```{r}
clean_data_tags <- tribble(
  ~acq_no, ~tags,
  "1999.32", "religious;portrait",
  "1908.54", "domestic scene;woman",
  "1955.32", "woman;window;portrait",
  "1955.33", "portrait"
)
kable(clean_data_tags)
```

- Standardize each individual tag
- Use a common delimiter to keep them separate

## Example 1: delimiters

If we encode this data correctly, then once we start processing it, we can easily "pivot" those data into the format we need to e.g. retrieve the paintings with the tag `woman`.

```{r, echo = TRUE}
clean_data_tags %>% 
  separate_rows(tags, sep = ";")
```


## Example 1: delimiters

Once it's in this format, we can now filter to get just the paintings we want based on tag.

```{r, echo = TRUE}
clean_data_tags %>% 
  separate_rows(tags, sep = ";") %>% 
  filter(tags == "woman")
```

## Example 2: Related table

Sometimes our objects make reference to things like people or places that, themselves, have many attributes.

In these cases, it's not enough to just use delimiters - we actually need to have a _related table_ that can hold this additional information without needing to repeat it again and again.

## Many-to-one

One museum can own many objects, but one object belongs to only one museum

```{r}
include_graphics("img/museum_object.png", dpi = 200)
```

```{r}
museums <- tribble(
  ~museum, ~city,
  "Rijksmuseum", "Amsterdam",
  "Victoria & Albert", "London",
  "British Museum", "London"
)
museums
```

## Many-to-one

We perform a _left join_ (you'll see this in R, python, SQL, pretty much everywhere) to copy attributes from the museums onto the objects.

```{r, echo = TRUE}
combined_data <- messy_data %>% 
  left_join(museums, by = "museum")
```

```{r}
combined_data %>% kable() %>% kable_styling(latex_options = "scale_down")
```

## Many-to-one

```{r, echo = TRUE}
london_paintings <- combined_data %>% 
  filter(city == "London")
```

```{r}
london_paintings %>% kable() %>% kable_styling(latex_options = "scale_down")
```

## Many-to-many

- Each object can have many artists
- Each artist can have many objects

We may well have biographical information about our artists

```{r}
artists <- tribble(
  ~name, ~birth_year,
  "Rembrandt", 1606,
  "Govaert Flinck", 1615,
  "Jan Vermeer", 1632,
  "Frans Hals", 1582
)
artists
```

## Many-to-many

This requires an intermediate table where we get to encode the relationship, and also encode variables _about_ that relationship.

```{r}
include_graphics("img/object_artist.png", dpi=250)
```

## Many-to-many

```{r}
creations <- tibble(
  acq_no = c("1999.32", "1999.32", "1908.54", "1955.32", "1955.33"),
  name = c("Rembrandt", "Govaert Flinck", "Jan Vermeer", "Jan Vermeer", "Frans Hals"),
  qualification = c("studio of", NA, NA, "possibly", NA)
)
creations
```

Note that painting `1999.32`, which has two artists, is repeated twice. And we can relate the qualifications (`studio of`, `possibly`) to specific artwork-artist pairs.

## Many-to-many

This lets us filter paintings based on their artists' biographical info

```{r, echo = TRUE}
temp_table <- artworks %>% 
  left_join(creations) %>% 
  left_join(artists)
```

```{r}
temp_table %>% kable() %>% kable_styling(latex_options = "scale_down")
```

## Querying on related tables

```{r, echo = TRUE}
date_filtered <- temp_table %>% 
  filter(birth_year <= 1615)
```

```{r}
date_filtered %>% kable() %>% kable_styling(latex_options = "scale_down")
```

# Uncertainty

## Uncertainty

There's a lot of uncertainty and missing information in historical sources.

What we can mostly handle are the known unknowns.

## Uncertainty DON'Ts

- Adding `[?]` into records won't tell you much
  - Was the info totally missing from the document?
  - Was that info there, but illegible?
  - Did the document literally say `[?]`?
- Mixing uncertainty across different fields
  - i.e. having a check mark to say a record is "done" isn't very informative
  - Which part of the record is uncertain? The date? The artist?

## Uncertainty DOs

- Make an uncertainty vocabulary if appropriate
  - missing
  - illegible
  - approximated
- Put boundaries on uncertainty
  - Dates aren't usually _totally_ unknown - what are the realistic early/late dates given context?
- Use separate columns liberally, e.g. `date`, `date_uncertainty`

You can't document everything! If some tricky field is just not relevant enough to your research, then don't kill yourself trying to capture it with perfect specificity.

## Be context-specific

```{r}
creations
```

- standardize your terms if you can
- only expend energy on it if it will meaningfully connect to your research question

## The "notes" column

There will always be info that doesn't fit into your schema. A "notes" column can be helpful here.

But as soon as you notice repeatedly putting a certain type of info in there, consider going back and making a dedicated column for that info.

Notes should usually have unique values. If they're often the same value, that means some of them should be moved to their own column.

# Only go as far as you need to

Data modeling can get infinitely complicated if you want to accommodate every possible use case. If you're not a museum or library, don't do that.

```{r photo_model, fig.cap="Photo archives data model from https://linked.art"}
include_graphics("img/photo_model.png", dpi=250)
```


# Bringing it all back together

```{r}
artworks <- messy_data %>% 
  select(acq_no, museum) %>% 
  left_join(clean_data_tags, by = "acq_no") %>% 
  left_join(clean_data_dates, by = "acq_no") %>% 
  select(-orig_date)
```

Most visualization and analysis software works with just one table.

By separating out tables first, we now have the flexibility to produce the one table we need for a given question.

```{r}
include_graphics("img/full_erd.png", dpi=250)
```

## Count up the different tags used

```{r, echo=TRUE}
artworks %>% 
  separate_rows(tags, sep=";") %>% 
  ggplot(aes(x = tags)) + 
  geom_bar()
```

## Tags based on year of creation

```{r, echo = TRUE}
artworks %>% 
  separate_rows(tags, sep=";") %>% 
  left_join(museums, by = "museum") %>% 
  ggplot(aes(ymin = year_early, ymax = year_late, 
             x = tags, color = city, group = acq_no)) +
  geom_errorbar(position="dodge", width = 0.2)
```

We need to decide how to handle year of creation. Some visualizations let us incorporate time-span info, but we could also take the midpoint between the boundaries, depending on our question.

## Network of artists who worked together and when

Joining a table to itself can give us combinations of artists who worked together on the same artwork. This could be used to create a network data set to analyze.

```{r, echo = TRUE}
creations %>% 
  left_join(creations, by = "acq_no") %>% 
  filter(name.x != name.y) %>% 
  left_join(artworks, by = "acq_no") %>% 
  select(acq_no, name.x, name.y, year_early, year_late)
```

# Practical exercise

Source: https://tinyurl.com/cmudh-2019-artcatalog

Group 1: https://tinyurl.com/cmudh-2019-tidy3

Group 2: https://tinyurl.com/cmudh-2019-tidy4

Working in groups, draft a data scheme for encoding this auction catalog. Aside from the obvious, think about:

- can we encode who's owned the artwork before?
- what are different ways to categorize the content of the descriptions?

# Linking data

Using shared vocabularies between data sets

- <https://vocab.getty.edu>
- <https://programminghistorian.org/en/lessons/intro-to-linked-data>

# Documenting tidy data

## Do it for future-you & for others

- You **will** forget what you did in a few months. Or even a few days. Docs will remind you.
- Docs make writing reports/articles easier
- Docs make your data reusable:
    - others won't have to guess at what a certain column means
    - or what decisions you made when recording it
    - or how to cite it
    - or if/how they may reuse it

## Show your work

- Describe what you made:
    - Keep a plain text doc in the same directory as your tables
    - Have a heading for each table
    - List every column name and describe what it means
        - Incl. list of possible values, relation to other tables as appropriate
- Document the process
    - Did you adapt this from another data set? (incl. original data, or link)
    - Describe the transformations you made, including what software you used

## Documentation format

A plain `.txt` file with a column name / definition list:

```
Table 1
-------
col 1 - definition
col 2 - joining id with table 2 - col 3
col 3 - definition


Table 2
-------
col 1 - definition
col 2 - definition
col 3 - joining id with table 1 - col 2
```

## Plain text

- Use plain text file types for tables and docs (`.txt`, `.csv`, not `.xslx`)
    - Free
    - Somewhat more future-proof
    - Track-able
- Creating in Excel/Google Sheets is fine, you can export it
    - When saving in Excel, use `UTF-8` so that accents & special characters are preserved
    - Don't rely on meaningful formatting (colored cells, bold, italics, borders), because that won't be preserved
    - Save multiple versions

## Archive it

- Bundle data and documentation in the same directory and zip them.
- Distribute
    - Institutional repository (upload it with your dissertation)
    - Journal websites
    - [Zenodo](https://zenodo.org/)
    - [Git](https://git-scm.com/) (works great with all-text files - more and more libraries and journals will be moving towards this method for tracking file versions)

# Resources

- [Building out tidy data using Google Sheets](https://matthewlincoln.net/2018/03/26/best-practices-for-using-google-sheets-in-your-data-project.html)
- [AirTable](https://airtable.com/) is a decent, Google-sheets-like option for building out multi-table relational databases.
- [See a tidy data example](https://github.com/mdlincoln/tidy-dh-data/tree/gh-pages/example_data)
- Database management
    - [UCLA DH101: Data and Databases](http://dh101.humanities.ucla.edu/?page_id=93)
    - [Designing Databases for Historical Research](http://port.sas.ac.uk/mod/book/view.php?id=75) (great intro to relational DBs)
- Data cleaning with [OpenRefine](http://programminghistorian.org/lessons/cleaning-data-with-openrefine)
